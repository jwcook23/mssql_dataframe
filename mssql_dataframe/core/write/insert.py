"""Methods for inserting data into SQL."""

from typing import Tuple, List
import string
import random

import pandas as pd
import pyodbc

from mssql_dataframe.core import custom_errors, conversion, modify, create
from mssql_dataframe.core.write import _exceptions


class insert:
    """Class for inserting data into SQL."""

    def __init__(
        self,
        connection: pyodbc.connect,
        include_metadata_timestamps: bool = False,
    ):
        """Class for inserting data into SQL.

        Parameters
        ----------
        connection (pyodbc.Connection) : connection for executing statement
        include_metadata_timestamps (bool, default=False) : include metadata timestamps _time_insert & _time_update for write operations
        """
        self._connection = connection
        self.include_metadata_timestamps = include_metadata_timestamps

        # create temporary tables for upsert/merging
        self._create = create.create(connection)

        # add include_metadata_timestamps if needed
        self._modify = modify.modify(connection)

    def insert(
        self,
        table_name: str,
        dataframe: pd.DataFrame,
        include_metadata_timestamps: bool = None,
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Insert data into SQL table from a dataframe.

        Parameters
        ----------
        table_name (str) : name of table to insert data into
        dataframe (pandas.DataFrame): tabular data to insert

        Returns
        -------
        dataframe (pandas.DataFrame) : input dataframe that may have been altered to conform to SQL

        Examples
        --------
        A sample table to insert a dataframe into.
        >>> create.table('##ExampleInsertDF', columns={'ColumnA': 'tinyint'}, not_nullable=['ColumnA'])

        Insert into the table. Include the column _time_insert (automatically created) to reflect in server time when the record was insert.
        >>> df = insert('##ExampleInsertDF', pd.DataFrame({'ColumnA': [1, 2, 3]}), include_metadata_timestamps=True)
        """
        # create cursor to perform operations
        cursor = self._connection.cursor()
        cursor.fast_executemany = True

        # override self.include_metadata_timestamps
        if include_metadata_timestamps is None:
            include_metadata_timestamps = self.include_metadata_timestamps

        # get target table schema, check/fix errors, and adjusting data for inserting
        if include_metadata_timestamps:
            additional_columns = ["_time_insert"]
        else:
            additional_columns = None
        schema, dataframe = self._target_table(
            table_name, dataframe, cursor, additional_columns
        )

        # insert dataframe values, dataframe values may be altered to conform to SQL precision limitations
        dataframe = conversion.insert_values(
            table_name, dataframe, include_metadata_timestamps, schema, cursor
        )

        return dataframe

    def _target_table(
        self,
        table_name: str,
        dataframe: pd.DataFrame,
        cursor: pyodbc.connect,
        additional_columns: List[str] = None,
        updating_table: bool = False,
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Get target schema, potentially handle errors, and adjust dataframe contents before inserting into target table.

        Parameters
        ----------
        table_name (str) : name of target table
        dataframe (pandas.DataFrame): tabular data to insert
        cursor (pyodbc.connection.cursor) : cursor to execute statement
        additional_columns (list, default=None) : columns that will be generated by an SQL statement but not in the dataframe
        updating_table (bool, default=False) : flag that indicates if target table is being updated

        Returns
        -------
        schema (pandas.DataFrame) : table column specifications and conversion rules
        dataframe (pandas.DataFrame) : input dataframe with optimal values and types for inserting into SQL
        """
        try:
            schema, dataframe = conversion.get_schema(
                self._connection,
                table_name,
                dataframe,
                additional_columns,
            )
        except custom_errors.SQLColumnDoesNotExist as failure:
            # add metadata_timestamps
            cursor.rollback()
            dataframe = _exceptions.add_metadata_timestamps(
                failure,
                table_name,
                dataframe,
                self._modify,
            )
            cursor.commit()
            # retry data insert
            schema, dataframe = conversion.get_schema(
                self._connection,
                table_name,
                dataframe,
                additional_columns,
            )
        except Exception as err:
            cursor.rollback()
            raise err

        return schema, dataframe

    def _column_spec(self, schema: pd.DataFrame, columns: list) -> dict:
        """Generate dictionary mapping of column name to SQL data type and specifications.

        Parameters
        ----------
        schema (pandas.DataFrame) : output of get_schema for a table

        Returns
        -------
        dtypes (dict) : dictionary mapping of each column name to SQL data type and specifications
        """
        # select only columns present in dataframe to prevent updating with nulls
        dtypes = schema.loc[
            columns, ["sql_category", "sql_type", "column_size", "decimal_digits"]
        ]
        dtypes = dtypes.astype("string")

        # add byte size for string columns
        idx = dtypes[dtypes["sql_category"] == "character string"].index
        dtypes.loc[idx, "sql_type"] = (
            dtypes.loc[idx, "sql_type"] + "(" + dtypes.loc[idx, "column_size"] + ")"
        )

        # add precision and scale for exact decimal numerics
        idx = dtypes[dtypes["sql_category"] == "exact_decimal_numeric"].index
        dtypes.loc[idx, "sql_type"] = (
            dtypes.loc[idx, "sql_type"]
            + "("
            + dtypes.loc[idx, "column_size"]
            + ","
            + dtypes.loc[idx, "decimal_digits"]
            + ")"
        )

        # add byte size for binary columns
        idx = dtypes[dtypes["sql_category"] == "binary"].index
        dtypes.loc[idx, "sql_type"] = (
            dtypes.loc[idx, "sql_type"] + "(" + dtypes.loc[idx, "column_size"] + ")"
        )

        # avoid creating an int identify data type column for a source table
        dtypes["sql_type"] = dtypes["sql_type"].replace("int identity", "int")

        # convert to dictionary of column name : data type
        dtypes = dtypes["sql_type"].to_dict()

        return dtypes

    def _source_table(
        self,
        table_name,
        dataframe,
        cursor,
        match_columns: list = None,
        additional_columns: list = None,
        updating_table: bool = False,
    ) -> Tuple[pd.DataFrame, pd.DataFrame, List[str], str]:
        """Create a source table with data in SQL for update and merge operations.

        Parameters
        ----------
        table_name (str) : name of target table
        dataframe (pandas.DataFrame): tabular data to insert
        cursor (pyodbc.connection.cursor) : cursor to execute statement
        match_columns (list|str) : columns to match records to updating/merging, if None the primary key is used
        additional_columns (list, default=None) : columns that will be generated by an SQL statement but not in the dataframe
        updating_table (bool, default=False) : flag that indicates if target table is being updated

        Returns
        -------
        schema (pandas.DataFrame) : table column specifications and conversion rules
        dataframe (pandas.DataFrame) : input dataframe with optimal values and types for inserting into SQL
        match_columns (list) : columns used to perform matching between source and target tables
        temp_name (str) : name of the source temporary table that was created

        """
        if isinstance(match_columns, str):
            match_columns = [match_columns]

        # get target table schema, while checking for errors and adjusting data for inserting
        schema, dataframe = self._target_table(
            table_name, dataframe, cursor, additional_columns, updating_table
        )

        # use primary key if match_columns is not given
        if match_columns is None:
            match_columns = list(schema[schema["pk_seq"].notna()].index)
            if not match_columns:
                raise custom_errors.SQLUndefinedPrimaryKey(
                    "SQL table {} has no primary key. Either set the primary key or specify the match_columns".format(
                        table_name
                    )
                )
        # match_column presence in dataframe
        missing = [
            x
            for x in match_columns
            if x not in list(dataframe.index.names) + list(dataframe.columns)
        ]
        if missing:
            raise custom_errors.DataframeColumnDoesNotExist(
                "match_columns not found in dataframe", missing
            )

        # insert data into source temporary table
        uid = "".join(random.choices(string.ascii_lowercase, k=4))  # nosec B311

        schema_name, table_name = conversion._get_schema_name(table_name)

        temp_name = f"##__source_{table_name}_{uid}"
        columns = list(dataframe.columns)
        if any(dataframe.index.names):
            columns = list(dataframe.index.names) + columns

        dtypes = self._column_spec(schema, columns)

        not_nullable = list(schema[~schema["is_nullable"]].index)

        self._create.table(
            temp_name, dtypes, not_nullable, primary_key_column=match_columns
        )

        dataframe = self.insert(temp_name, dataframe, include_metadata_timestamps=False)

        # reset match columns that were part of the primary key in the source table
        # dataframe needs returned in the event values were adjusted but indicies/columns should be the same
        names = schema.loc[dataframe.index.names, "pk_seq"]
        extra = names[names.isna()].index.tolist()
        if any(extra):
            dataframe = dataframe.reset_index(level=extra)

        return schema, dataframe, match_columns, temp_name, schema_name, table_name
